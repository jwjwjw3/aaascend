{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2dfc7fd2-c8c3-432c-9f0a-ee1b4895b1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import vai_q_onnx\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "from onnxruntime.quantization import CalibrationDataReader, QuantType, QuantFormat, CalibrationMethod, quantize_static"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69dc8eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import urllib.request\n",
    "from resnet_utils import get_directories\n",
    "\n",
    "_, models_dir, data_dir, _ = get_directories()\n",
    "data_download_path_python = data_dir / \"cifar-10-python.tar.gz\"\n",
    "data_download_path_bin = data_dir / \"cifar-10-binary.tar.gz\"\n",
    "if not data_download_path_python.exists() or not data_download_path_bin.exists():\n",
    "    urllib.request.urlretrieve(\"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\", data_download_path_python)\n",
    "    urllib.request.urlretrieve(\"https://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz\", data_download_path_bin)\n",
    "    file_python = tarfile.open(data_download_path_python)\n",
    "    file_python.extractall(data_dir)\n",
    "    file_python.close()\n",
    "    file_bin = tarfile.open(data_download_path_bin)\n",
    "    file_bin.extractall(data_dir)\n",
    "    file_bin.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7aebaac4-9b52-43b6-81c8-bd077ae118ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR10DataSet:\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.train_path = data_dir\n",
    "        self.vld_path = data_dir\n",
    "        self.setup(\"fit\")\n",
    "\n",
    "    def setup(self, stage: str):\n",
    "        transform = transforms.Compose(\n",
    "            [transforms.Pad(4), transforms.RandomHorizontalFlip(), transforms.RandomCrop(32), transforms.ToTensor()]\n",
    "        )\n",
    "        self.train_dataset = CIFAR10(root=self.train_path, train=True, transform=transform, download=False)\n",
    "        self.val_dataset = CIFAR10(root=self.vld_path, train=True, transform=transform, download=False)\n",
    "\n",
    "\n",
    "class PytorchResNetDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.dataset[index]\n",
    "        input_data = sample[0]\n",
    "        label = sample[1]\n",
    "        return input_data, label\n",
    "\n",
    "\n",
    "class ResnetCalibrationDataReader(CalibrationDataReader):\n",
    "    def __init__(self, data_dir: str, batch_size: int = 16):\n",
    "        super().__init__()\n",
    "        cifar10_dataset = CIFAR10DataSet(data_dir)\n",
    "        _, val_set = torch.utils.data.random_split(cifar10_dataset.val_dataset, [49000, 1000])\n",
    "        self.iterator = iter(DataLoader(PytorchResNetDataset(val_set), batch_size=batch_size, drop_last=True))\n",
    "\n",
    "    def get_next(self) -> dict:\n",
    "        try:\n",
    "            images, labels = next(self.iterator)\n",
    "            return {\"input\": images.numpy()}\n",
    "        except Exception:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9103c4cd-606d-480b-ab3b-1ed0f989547d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_model_dir = 'models/onnx'\n",
    "quant_model_dir = 'models/quant'\n",
    "\n",
    "calibration_dataset_path = \"data/\"\n",
    "\n",
    "import os\n",
    "models = os.listdir(input_model_dir)\n",
    "quantized_models = os.listdir(quant_model_dir)\n",
    "if not len(models) == len(quantized_models):\n",
    "    for model in models:\n",
    "        model_name = model.split('.')[0]\n",
    "        print(model_name)\n",
    "\n",
    "        input_model_path = os.path.join(input_model_dir, model_name + '.onnx')\n",
    "        output_model_path = os.path.join(quant_model_dir, model_name + '.U8S8.onnx')\n",
    "\n",
    "        dr = ResnetCalibrationDataReader(calibration_dataset_path, batch_size=16)\n",
    "\n",
    "        vai_q_onnx.quantize_static(\n",
    "            input_model_path,\n",
    "            output_model_path,\n",
    "            dr,\n",
    "            quant_format=vai_q_onnx.QuantFormat.QDQ,\n",
    "            calibrate_method=vai_q_onnx.PowerOfTwoMethod.MinMSE,\n",
    "            activation_type=vai_q_onnx.QuantType.QUInt8,\n",
    "            weight_type=vai_q_onnx.QuantType.QInt8,\n",
    "            enable_dpu=True, \n",
    "            extra_options={'ActivationSymmetric': True} \n",
    "        )\n",
    "\n",
    "        print('Calibrated and quantized model saved at:', output_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "450090f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file,'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='latin1')\n",
    "    return dict\n",
    "\n",
    "datafile = r'./data/cifar-10-batches-py/test_batch'\n",
    "metafile = r'./data/cifar-10-batches-py/batches.meta'\n",
    "\n",
    "data_batch_1 = unpickle(datafile) \n",
    "metadata = unpickle(metafile)\n",
    "\n",
    "images = data_batch_1['data']\n",
    "labels = data_batch_1['labels']\n",
    "images = np.reshape(images,(10000, 3, 32, 32))\n",
    "\n",
    "dirname = 'images'\n",
    "if not os.path.exists(dirname):\n",
    "   os.mkdir(dirname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5eada65e-fdcf-4232-84fe-93e4b29df1d0",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "quantized_model_path = r'./models/resnet.qdq.U8S8.onnx'\n",
    "model = onnx.load(quantized_model_path)\n",
    "\n",
    "use_aie = True\n",
    "\n",
    "providers = ['CPUExecutionProvider']\n",
    "provider_options = [{}]\n",
    "\n",
    "if use_aie:\n",
    "   providers = ['VitisAIExecutionProvider']\n",
    "   cache_dir = './'\n",
    "   provider_options = [{\n",
    "                'config_file': 'vaip_config.json',\n",
    "                'cacheDir': str(cache_dir),\n",
    "                'cacheKey': 'modelcachekey'\n",
    "            }]\n",
    "\n",
    "session = ort.InferenceSession(model.SerializeToString(), providers=providers,\n",
    "                               provider_options=provider_options)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2219fd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Extract and dump first 10 images\n",
    "correct = 0\n",
    "img0 = images[0]\n",
    "print(img0.shape)\n",
    "print(img0[0][0])\n",
    "for i in range(100): \n",
    "    im = images[i]\n",
    "    # im = cv2.cvtColor(im,cv2.COLOR_RGB2BGR)\n",
    "    # im_name = f'./images/image_{i}.png'\n",
    "    # cv2.imwrite(im_name, im)\n",
    "\n",
    "    # image_name = f'./images/image_{i}.png'\n",
    "    # image = Image.open(image_name).convert('RGB')\n",
    "    # Resize the image to match the input size expected by the model\n",
    "    # image = im.resize((32, 32))  \n",
    "    image_array = np.array(im).astype(np.float32)\n",
    "    image_array = image_array/255\n",
    "\n",
    "    # Reshape the array to match the input shape expected by the model\n",
    "\n",
    "    # Add a batch dimension to the input image\n",
    "    input_data = np.expand_dims(image_array, axis=0)\n",
    "\n",
    "    # Run the model\n",
    "    outputs = session.run(None, {'input': input_data})\n",
    "\n",
    "    # Process the outputs\n",
    "    output_array = outputs[0]\n",
    "    predicted_class = np.argmax(output_array)\n",
    "    predicted_label = metadata['label_names'][predicted_class]\n",
    "    label = metadata['label_names'][labels[i]]\n",
    "    if predicted_class == labels[i]:\n",
    "        correct += 1\n",
    "    # print(f'Image {i}: Actual Label {label}, Predicted Label {predicted_label}')\n",
    "\n",
    "print(correct)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
