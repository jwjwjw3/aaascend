{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2dfc7fd2-c8c3-432c-9f0a-ee1b4895b1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import vai_q_onnx\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "from onnxruntime.quantization import CalibrationDataReader, QuantType, QuantFormat, CalibrationMethod, quantize_static"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69dc8eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import urllib.request\n",
    "from resnet_utils import get_directories\n",
    "\n",
    "_, models_dir, data_dir, _ = get_directories()\n",
    "if not data_dir.exists():\n",
    "    data_download_path_python = data_dir / \"cifar-10-python.tar.gz\"\n",
    "    data_download_path_bin = data_dir / \"cifar-10-binary.tar.gz\"\n",
    "    urllib.request.urlretrieve(\"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\", data_download_path_python)\n",
    "    urllib.request.urlretrieve(\"https://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz\", data_download_path_bin)\n",
    "    file_python = tarfile.open(data_download_path_python)\n",
    "    file_python.extractall(data_dir)\n",
    "    file_python.close()\n",
    "    file_bin = tarfile.open(data_download_path_bin)\n",
    "    file_bin.extractall(data_dir)\n",
    "    file_bin.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7aebaac4-9b52-43b6-81c8-bd077ae118ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR10DataSet:\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.train_path = data_dir\n",
    "        self.vld_path = data_dir\n",
    "        self.setup(\"fit\")\n",
    "\n",
    "    def setup(self, stage: str):\n",
    "        transform = transforms.Compose(\n",
    "            [transforms.Pad(4), transforms.RandomHorizontalFlip(), transforms.RandomCrop(32), transforms.ToTensor()]\n",
    "        )\n",
    "        self.train_dataset = CIFAR10(root=self.train_path, train=True, transform=transform, download=False)\n",
    "        self.val_dataset = CIFAR10(root=self.vld_path, train=True, transform=transform, download=False)\n",
    "\n",
    "\n",
    "class PytorchResNetDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.dataset[index]\n",
    "        input_data = sample[0]\n",
    "        label = sample[1]\n",
    "        return input_data, label\n",
    "\n",
    "\n",
    "class ResnetCalibrationDataReader(CalibrationDataReader):\n",
    "    def __init__(self, data_dir: str, batch_size: int = 16):\n",
    "        super().__init__()\n",
    "        cifar10_dataset = CIFAR10DataSet(data_dir)\n",
    "        _, val_set = torch.utils.data.random_split(cifar10_dataset.val_dataset, [49000, 1000])\n",
    "        self.iterator = iter(DataLoader(PytorchResNetDataset(val_set), batch_size=batch_size, drop_last=True))\n",
    "\n",
    "    def get_next(self) -> dict:\n",
    "        try:\n",
    "            images, labels = next(self.iterator)\n",
    "            return {\"input\": images.numpy()}\n",
    "        except Exception:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9103c4cd-606d-480b-ab3b-1ed0f989547d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:vai_q_onnx.quantize:The 'enable_dpu' parameter will be deprecated in future versions. Please use 'enable_ipu_cnn' instead.\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/onnx/ResNet10_1111_4.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 62 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/onnx/ResNet10_1111_4.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-04-14 16:56:17.554112\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- GEEKOM-AMD\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.0\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/onnx/ResNet10_1111_4.onnx\n",
      "                                  model_output --- models/quant/ResNet10_1111_4.U8S8.onnx\n",
      "                       calibration_data_reader --- <__main__.ResnetCalibrationDataReader object at 0x000001DA48C68730>\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "Computing range: 100%|██████████| 30/30 [00:05<00:00,  5.63tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "INFO:vai_q_onnx.simulate_dpu:Rescale AveragePool /AveragePool with factor 1.0 to simulate DPU behavior.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model                            </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 13                                     </span>│\n",
       "│ Relu                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 9                                      </span>│\n",
       "│ Add                  │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 4                                      </span>│\n",
       "│ AveragePool          │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                                      </span>│\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                                      </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 3                                      </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                                      </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                                      </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                                      </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                                      </span>│\n",
       "│ Gemm                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                                      </span>│\n",
       "├──────────────────────┼────────────────────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/quant/ResNet10_1111_4.U8S8.onnx </span>│\n",
       "└──────────────────────┴────────────────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model                           \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m13                                    \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Relu                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m9                                     \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Add                  │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m4                                     \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ AveragePool          │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                                     \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                                     \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m3                                     \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                                     \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                                     \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                                     \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                                     \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gemm                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                                     \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼────────────────────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/quant/ResNet10_1111_4.U8S8.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴────────────────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibrated and quantized model saved at: models/quant/ResNet10_1111_4.U8S8.onnx\n"
     ]
    }
   ],
   "source": [
    "input_model_path = \"models/onnx/ResNet10_1111_4.onnx\"\n",
    "output_model_path = \"models/quant/ResNet10_1111_4.U8S8.onnx\"\n",
    "calibration_dataset_path = \"data/\"\n",
    "\n",
    "dr = ResnetCalibrationDataReader(calibration_dataset_path, batch_size=16)\n",
    "\n",
    "vai_q_onnx.quantize_static(\n",
    "    input_model_path,\n",
    "    output_model_path,\n",
    "    dr,\n",
    "    quant_format=vai_q_onnx.QuantFormat.QDQ,\n",
    "    calibrate_method=vai_q_onnx.PowerOfTwoMethod.MinMSE,\n",
    "    activation_type=vai_q_onnx.QuantType.QUInt8,\n",
    "    weight_type=vai_q_onnx.QuantType.QInt8,\n",
    "    enable_dpu=True, \n",
    "    extra_options={'ActivationSymmetric': True} \n",
    ")\n",
    "\n",
    "print('Calibrated and quantized model saved at:', output_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5eada65e-fdcf-4232-84fe-93e4b29df1d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image 0: Actual Label cat, Predicted Label cat\n",
      "Image 1: Actual Label ship, Predicted Label cat\n",
      "Image 2: Actual Label ship, Predicted Label cat\n",
      "Image 3: Actual Label airplane, Predicted Label cat\n",
      "Image 4: Actual Label frog, Predicted Label cat\n",
      "Image 5: Actual Label frog, Predicted Label cat\n",
      "Image 6: Actual Label automobile, Predicted Label horse\n",
      "Image 7: Actual Label frog, Predicted Label cat\n",
      "Image 8: Actual Label cat, Predicted Label cat\n",
      "Image 9: Actual Label automobile, Predicted Label cat\n"
     ]
    }
   ],
   "source": [
    "quantized_model_path = r'models/quant/ResNet10_1111_4.U8S8.onnx'\n",
    "model = onnx.load(quantized_model_path)\n",
    "\n",
    "\n",
    "providers = ['CPUExecutionProvider']\n",
    "provider_options = [{}]\n",
    "\n",
    "use_aie = True\n",
    "if use_aie:\n",
    "   providers = ['VitisAIExecutionProvider']\n",
    "   cache_dir = './'\n",
    "   provider_options = [{\n",
    "                'config_file': 'vaip_config.json',\n",
    "                'cacheDir': str(cache_dir),\n",
    "                'cacheKey': 'modelcachekey'\n",
    "            }]\n",
    "\n",
    "session = ort.InferenceSession(model.SerializeToString(), providers=providers,\n",
    "                               provider_options=provider_options)\n",
    "\n",
    "\n",
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file,'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='latin1')\n",
    "    return dict\n",
    "\n",
    "datafile = r'./data/cifar-10-batches-py/test_batch'\n",
    "metafile = r'./data/cifar-10-batches-py/batches.meta'\n",
    "\n",
    "data_batch_1 = unpickle(datafile) \n",
    "metadata = unpickle(metafile)\n",
    "\n",
    "images = data_batch_1['data']\n",
    "labels = data_batch_1['labels']\n",
    "images = np.reshape(images,(10000, 3, 32, 32))\n",
    "\n",
    "import os\n",
    "dirname = 'images'\n",
    "if not os.path.exists(dirname):\n",
    "   os.mkdir(dirname)\n",
    "\n",
    "\n",
    "#Extract and dump first 10 images \n",
    "for i in range (0,10): \n",
    "    im = images[i]\n",
    "    im  = im.transpose(1,2,0)\n",
    "    im = cv2.cvtColor(im,cv2.COLOR_RGB2BGR)\n",
    "    im_name = f'./images/image_{i}.png'\n",
    "    cv2.imwrite(im_name, im)\n",
    "\n",
    "#Pick dumped images and predict\n",
    "for i in range (0,10): \n",
    "    image_name = f'./images/image_{i}.png'\n",
    "    image = Image.open(image_name).convert('RGB')\n",
    "    # Resize the image to match the input size expected by the model\n",
    "    image = image.resize((32, 32))  \n",
    "    image_array = np.array(image).astype(np.float32)\n",
    "    image_array = image_array/255\n",
    "\n",
    "    # Reshape the array to match the input shape expected by the model\n",
    "    image_array = np.transpose(image_array, (2, 0, 1))  \n",
    "\n",
    "    # Add a batch dimension to the input image\n",
    "    input_data = np.expand_dims(image_array, axis=0)\n",
    "\n",
    "\n",
    "    # Run the model\n",
    "    outputs = session.run(None, {'input': input_data})\n",
    "\n",
    "\n",
    "    # Process the outputs\n",
    "    output_array = outputs[0]\n",
    "    predicted_class = np.argmax(output_array)\n",
    "    predicted_label = metadata['label_names'][predicted_class]\n",
    "    label = metadata['label_names'][labels[i]]\n",
    "    print(f'Image {i}: Actual Label {label}, Predicted Label {predicted_label}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2219fd5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
